Hugging Face, Foundation models


![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face

  * Models
  * Datasets
  * Spaces
  * Docs
  * Solutions 

  * Pricing 
  *   * * * *

  * Log In 
  * Sign Up 

TRL documentation

TRL - Transformer Reinforcement Learning

#

TRL

Search documentation

mainv0.7.4v0.6.0v0.5.0v0.4.7v0.3.1v0.2.1v0.1.1 EN

Get started

TRL Quickstart Installation PPO Training FAQ Use Trained Models Customize the
Training Understanding Logs

API

Model Classes Trainer Classes Reward Model Training Supervised Fine-Tuning PPO
Trainer Best of N Sampling DPO Trainer Denoising Diffusion Policy Optimization
Iterative Supervised Fine-Tuning Text Environments

Examples

Example Overview Sentiment Tuning Training with PEFT Detoxifying a Language
Model Training StackLlama Learning to Use Tools Multi Adapter RLHF

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

Sign Up

to get started

![](https://huggingface.co/datasets/trl-internal-testing/example-
images/resolve/main/images/trl_banner_dark.png)

#  TRL - Transformer Reinforcement Learning

TRL is a full stack library where we provide a set of tools to train
transformer language models with Reinforcement Learning, from the Supervised
Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy
Optimization (PPO) step. The library is integrated with ðŸ¤— transformers.

![](https://huggingface.co/datasets/trl-internal-testing/example-
images/resolve/main/images/TRL-readme.png)

Check the appropriate sections of the documentation depending on your needs:

##  API documentation

  * Model Classes: _A brief overview of what each public model class does._
  * `SFTTrainer`: _Supervise Fine-tune your model easily with`SFTTrainer`_
  * `RewardTrainer`: _Train easily your reward model using`RewardTrainer`._
  * `PPOTrainer`: _Further fine-tune the supervised fine-tuned model using PPO algorithm_
  * Best-of-N Sampling: _Use best of n sampling as an alternative way to sample predictions from your active model_
  * `DPOTrainer`: _Direct Preference Optimization training using`DPOTrainer`._
  * `TextEnvironment`: _Text environment to train your model using tools with RL._

##  Examples

  * Sentiment Tuning: _Fine tune your model to generate positive movie contents_
  * Training with PEFT: _Memory efficient RLHF training using adapters with PEFT_
  * Detoxifying LLMs: _Detoxify your language model through RLHF_
  * StackLlama: _End-to-end RLHF training of a Llama model on Stack exchange dataset_
  * Learning with Tools: _Walkthrough of using`TextEnvironments`_
  * Multi-Adapter Training: _Use a single base model and multiple adapters for memory efficient end-to-end training_

##  Blog posts

![thumbnail](https://raw.githubusercontent.com/huggingface/blog/main/assets/120_rlhf/thumbnail.png)

Illustrating Reinforcement Learning from Human Feedback

![thumbnail](https://github.com/huggingface/blog/blob/main/assets/133_trl_peft/thumbnail.png?raw=true)

Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU

![thumbnail](https://github.com/huggingface/blog/blob/main/assets/138_stackllama/thumbnail.png?raw=true)

StackLLaMA: A hands-on guide to train LLaMA with RLHF

![thumbnail](https://github.com/huggingface/blog/blob/main/assets/157_dpo_trl/dpo_thumbnail.png?raw=true)

Fine-tune Llama 2 with DPO

![thumbnail](https://github.com/huggingface/blog/blob/main/assets/166_trl_ddpo/thumbnail.png?raw=true)

Finetune Stable Diffusion Models with DDPO via TRL

Quickstartâ†’

TRL - Transformer Reinforcement Learning API documentation Examples Blog posts

