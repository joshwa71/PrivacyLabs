Hugging Face, Foundation models


![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face

  * Models
  * Datasets
  * Spaces
  * Docs
  * Solutions 

  * Pricing 
  *   * * * *

  * Log In 
  * Sign Up 

PEFT documentation

PEFT

#

PEFT

Search documentation

mainv0.6.2 EN

Get started

ðŸ¤— PEFT Quicktour Installation

Task guides

Image classification using LoRA Prefix tuning for conditional generation
Prompt tuning for causal language modeling Semantic segmentation using LoRA
P-tuning for sequence classification Dreambooth fine-tuning with LoRA LoRA for
token classification int8 training for automatic speech recognition Semantic
similarity with LoRA

Developer guides

Working with custom models PEFT low level API Contributing to PEFT
Troubleshooting

ðŸ¤— Accelerate integrations

DeepSpeed Fully Sharded Data Parallel

Conceptual guides

LoRA Prompting IA3

Reference

PEFT model Configuration Tuners

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

Sign Up

to get started

#  PEFT

ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for
efficiently adapting pre-trained language models (PLMs) to various downstream
applications without fine-tuning all the modelâ€™s parameters. PEFT methods only
fine-tune a small number of (extra) model parameters, significantly decreasing
computational and storage costs because fine-tuning large-scale PLMs is
prohibitively costly. Recent state-of-the-art PEFT techniques achieve
performance comparable to that of full fine-tuning.

PEFT is seamlessly integrated with ðŸ¤— Accelerate for large-scale models
leveraging DeepSpeed and Big Model Inference.

Get started

Start here if you're new to ðŸ¤— PEFT to get an overview of the library's main
features, and how to train a model with a PEFT method.

How-to guides

Practical guides demonstrating how to apply various PEFT methods across
different types of tasks like image classification, causal language modeling,
automatic speech recognition, and more. Learn how to use ðŸ¤— PEFT with the
DeepSpeed and Fully Sharded Data Parallel scripts.

Conceptual guides

Get a better theoretical understanding of how LoRA and various soft prompting
methods help reduce the number of trainable parameters to make training more
efficient.

Reference

Technical descriptions of how ðŸ¤— PEFT classes and methods work.

##  Supported methods

  1. LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
  2. Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
  3. P-Tuning: GPT Understands, Too
  4. Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning
  5. AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning
  6. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention
  7. IA3: Infused Adapter by Inhibiting and Amplifying Inner Activations

##  Supported models

The tables provided below list the PEFT methods and models supported for each
task. To apply a particular PEFT method for a task, please refer to the
corresponding Task guides.

###  Causal Language Modeling

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
GPT-2 | âœ… | âœ… | âœ… | âœ… | âœ…  
Bloom | âœ… | âœ… | âœ… | âœ… | âœ…  
OPT | âœ… | âœ… | âœ… | âœ… | âœ…  
GPT-Neo | âœ… | âœ… | âœ… | âœ… | âœ…  
GPT-J | âœ… | âœ… | âœ… | âœ… | âœ…  
GPT-NeoX-20B | âœ… | âœ… | âœ… | âœ… | âœ…  
LLaMA | âœ… | âœ… | âœ… | âœ… | âœ…  
ChatGLM | âœ… | âœ… | âœ… | âœ… | âœ…  
  
###  Conditional Generation

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
T5 | âœ… | âœ… | âœ… | âœ… | âœ…  
BART | âœ… | âœ… | âœ… | âœ… | âœ…  
  
###  Sequence Classification

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
BERT | âœ… | âœ… | âœ… | âœ… | âœ…  
RoBERTa | âœ… | âœ… | âœ… | âœ… | âœ…  
GPT-2 | âœ… | âœ… | âœ… | âœ… |  
Bloom | âœ… | âœ… | âœ… | âœ… |  
OPT | âœ… | âœ… | âœ… | âœ… |  
GPT-Neo | âœ… | âœ… | âœ… | âœ… |  
GPT-J | âœ… | âœ… | âœ… | âœ… |  
Deberta | âœ… |  | âœ… | âœ… |  
Deberta-v2 | âœ… |  | âœ… | âœ… |  
  
###  Token Classification

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
BERT | âœ… | âœ… |  |  |  
RoBERTa | âœ… | âœ… |  |  |  
GPT-2 | âœ… | âœ… |  |  |  
Bloom | âœ… | âœ… |  |  |  
OPT | âœ… | âœ… |  |  |  
GPT-Neo | âœ… | âœ… |  |  |  
GPT-J | âœ… | âœ… |  |  |  
Deberta | âœ… |  |  |  |  
Deberta-v2 | âœ… |  |  |  |  
  
###  Text-to-Image Generation

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
Stable Diffusion | âœ… |  |  |  |  
  
###  Image Classification

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3 |  
---|---|---|---|---|---|---  
ViT | âœ… |  |  |  |  |  
Swin | âœ… |  |  |  |  |  
  
###  Image to text (Multi-modal models)

We have tested LoRA for ViT and Swin for fine-tuning on image classification.
However, it should be possible to use LoRA for any ViT-based model from ðŸ¤—
Transformers. Check out the Image classification task guide to learn more. If
you run into problems, please open an issue.

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
Blip-2 | âœ… |  |  |  |  
  
###  Semantic Segmentation

As with image-to-text models, you should be able to apply LoRA to any of the
segmentation models. Itâ€™s worth noting that we havenâ€™t tested this with every
architecture yet. Therefore, if you come across any issues, kindly create an
issue report.

Model | LoRA | Prefix Tuning | P-Tuning | Prompt Tuning | IA3  
---|---|---|---|---|---  
SegFormer | âœ… |  |  |  |  
  
Quicktourâ†’

PEFT Supported methods Supported models Causal Language Modeling Conditional
Generation Sequence Classification Token Classification Text-to-Image
Generation Image Classification Image to text (Multi-modal models) Semantic
Segmentation

